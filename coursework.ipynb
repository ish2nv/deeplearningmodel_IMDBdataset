{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The purpose of this report is to investigate the binary classification problem, how I based my deep learning model on this problem, the code I implemented to achieve this shown step-by-step and the results of it expressed via graphs and tables. The results will output the experiments I've done in the workflow. The experiments will attempt to improve the networks precision by tackling whatever problems that I identify. I will also investigate the different hyperparameter settings and adjust certain hyperparameters so the model can be at an optimal level. As for the final part of my report, I will devise a conclusion of all my findings in a paragraph. I have chosen the IMDB dataset as my example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "This section will consist of my workflow, where I will attempt to build a deep learning model using the IMDB dataset for the binary classification problem. I will go through this step-by-step in my workflow. Explanation of each step will be in comments within the code. The general aim of the binary classification problem is to classify elements from the training set and test set into two classes. I will also perform a series of experiments, where I will test on a small model and a large model to analyse their performances. For example, analysing the validation set can assist me in investigating how well or bad the model has fit and whether I should adjust my hyperparameters.\n",
    "\n",
    "# Workflow\n",
    "\n",
    "## Preparing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb     #importing the IMDB dataset\n",
    "import numpy as np                  \n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt   #will allow us to create graphs and table\n",
    "from keras import regularizers    #regularizers will allow us to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((trainData, trainLabels), (testData, testLabels)) = imdb.load_data(num_words = 7500) #this keeps the top 7500 most recurrent words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(sequences, dimension = 7500): #this function will reshape the trainData and testData to (25000,7500.) \n",
    "                                          #25,000 sets with 7500 elements in each. This will be set to a new variable as we do \n",
    "                                          #not need to directly change the shape of trainData and testData \n",
    "            \n",
    "    results = np.zeros( (len(sequences), dimension) )  #creating an array of zeros in shape (25000,7500)\n",
    "    \n",
    "    for i, sequence in enumerate(sequences):  # To simplify this for loop, lets picture all numbers within each set of array acting as\n",
    "        results[i,sequence] = 1.              # index numbers. These index numbers will be the location of all the zeros that will turn\n",
    "                                              # into ones within the set of array you are in                                             \n",
    " \n",
    "    return results                            #returns the reshaped array containing 1's and 0's \n",
    "                                              #i.e. the two classes required for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh = np.array([[1,2,3,4],     \n",
    "               [2,2,1,3],\n",
    "               [0,2,4,3]])\n",
    "\n",
    "print(one_hot(hh,5)) #Here is an example of how the one_hot encoding works with a small dimension of 5\n",
    "\n",
    "\n",
    "trainD = one_hot(trainData)   #we will assign it to a new variable\n",
    "testD = one_hot(testData)\n",
    "trainL = np.asarray(trainLabels).astype('float32')  #integers within the trainLabels and testLabels array are now floating point\n",
    "testL = np.asarray(testLabels).astype('float32')    #working with floating points can give us more accurate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Building our deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(8, activation = 'relu', input_shape = (7500,)))  #Dense simply means units in the layers are fully connected\n",
    "model.add(layers.Dense(8, activation = 'relu')) #relu activation function follows this simple formula max(0,x) it is a very beneficial function that can reduce the likeliness of the gradient to disappear  \n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))  #sigmoid activation function helps create an output in range of 0 and 1. This is very useful for binary classification problem\n",
    "#experiment with small units\n",
    "\n",
    "\n",
    "model.compile(optimizer = optimizers.RMSprop(lr = 0.01), #a gradient based optimization technique. this is a hyperparameter we can adjust\n",
    "              loss = 'binary_crossentropy',  #ideal loss function for binary classification problem\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "history1 = model.fit(trainD,                      #training data\n",
    "                    trainL,                     #training label\n",
    "                    validation_split = 0.4,  #validation set will be 20% of the training set i.e. 5000 samples\n",
    "                    epochs = 15,          \n",
    "                    batch_size =700)  #in each epoch, the batch size will iterate 25000/700 times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLoss,testAcc = model.evaluate(testD, testL)    #test set\n",
    "print(\"loss\", testLoss)\n",
    "print(\"Acc\", testAcc)\n",
    "\n",
    "model.predict(testD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Large Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(70, activation = 'relu', input_shape = (7500,))) \n",
    "model.add(layers.Dense(70, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))  \n",
    "#experiment with large units\n",
    "\n",
    "\n",
    "model.compile(optimizer = optimizers.RMSprop(lr = 0.01),\n",
    "              loss = 'binary_crossentropy', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "history2 = model.fit(trainD,                      \n",
    "                    trainL,\n",
    "                    validation_split = 0.40,\n",
    "                    epochs = 15,\n",
    "                    batch_size =700)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLoss,testAcc = model.evaluate(testD, testL)    #test set\n",
    "print(\"loss\", testLoss)\n",
    "print(\"Acc\", testAcc)\n",
    "\n",
    "model.predict(testD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate different hyperparameter settings\n",
    "\n",
    "Hyperparameters are parameters or variables that are set before training begins. It can shape the network structure and controls how the network should be trained. Just below is a list of different hyperparameters that we can find in Keras.\n",
    "\n",
    "<b>Learning Rate:</b> manages how much we are changing the weights of the network with regards to the loss gradient\n",
    "\n",
    "<b>Batch Size:</b> the whole dataset cannot be passed into the network at once, so we split the dataset into batches. For example, the dataset I'm using will be 25,000. And let's say our batch size is 700. In each epoch, the batch size will iterate 25000/700 times.\n",
    "\n",
    "<b>Number of hidden layers:</b> correct number of layers should prevent overfitting, but too many layers can risk overfitting.\n",
    "\n",
    "<b>Units in layers:</b> the more units you add to each layer, the number of learnable parameters increase in the model. The correct number of units will depend on what kind of neural network your using\n",
    "\n",
    "<b>Optimizer:</b> an optimization algorithm is something that is executed repeatedly by comparing different solutions until the best solution is found. Examples in Keras include RMSprop, Adam, SGD (stochastic gradient descent) etc.\n",
    "\n",
    "<b>Epoch:</b> entire training data passed from the network\n",
    "\n",
    "<b>Activation function:</b> it interprets the output of a neuron after given an input or inputs. Then, that output is used as an input for the following neuron. This repeats on and on until a optimal solution is found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjustments made to the hyperparameters\n",
    "\n",
    "Below, their is a table showing all the adjustments I will make to the hyperparameters for experiment 2. I have researched and investigated through trial and error as to which will be the best changes for my model.\n",
    "\n",
    "\n",
    "\n",
    "|  Hyperparameters    | Experiment 1| Experiment 2   |Reason         |\n",
    "|------|------|------|------|\n",
    "| Learning Rate:|0.01| 0.0007|The learning rate was slightly too high for the first experiment, causing <br> the deep learning model to behave undesirably (can be seen clearly <br>in the graphs under results section.)      |\n",
    "|   Batch Size:   | 700| 200  |Reducing to mini-batch sizes can cut down on total training time|\n",
    "|   Units in the layers for small model:   | 8| 12  |To investigate how it performs in a slighter bigger, but small network <br> capacity              |\n",
    "|   Units in the layers for large model:   | 70| 80  |To investigate how it performs in a bigger network capacity      |\n",
    "|   Optimizer:   | RMSprop| Adamax  |Based on trial and error with different optimizers, Adamax worked the <br> best out of all the others |\n",
    "|   Epochs:   | 15| 30  |Epochs have been increased for the next experiment so I can find out <br> if the validation set begins to degrade after going through a greater <br> number of epochs.                |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Adjusting hyperparameters and applying regularization\n",
    "\n",
    "#### Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(12, activation = 'relu', input_shape = (7500,),kernel_regularizer = regularizers.l2(0.000004))) #adjusted hyperparameter and applied L2 regularization\n",
    "model.add(layers.Dense(12,activation = 'relu',kernel_regularizer = regularizers.l2(0.3))) #adjusted hyperparameter and applied L2 regularization\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))  \n",
    "#experiment with small units\n",
    "\n",
    "\n",
    "model.compile(optimizer = optimizers.Adamax(lr = 0.0007), #adjusted hyperparameters\n",
    "              loss = 'binary_crossentropy', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "history3 = model.fit(trainD, \n",
    "                    trainL,\n",
    "                    validation_split = 0.4,\n",
    "                    epochs = 30, #adjusted hyperparameter\n",
    "                    batch_size =200)  #adjusted hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLoss,testAcc = model.evaluate(testD, testL)    #test set\n",
    "print(\"loss\", testLoss)\n",
    "print(\"Acc\", testAcc)\n",
    "\n",
    "model.predict(testD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set for the small model in this experiment has improved. The loss has reduced drastically because in experiment 1, the test loss was ≈ 0.7, but in our second try the test loss has now minimized to ≈ 0.34."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Large Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(80, activation = 'relu', input_shape = (7500,),kernel_regularizer = regularizers.l2(0.000004))) #adjusted hyperparameter and applied L2 regularization\n",
    "model.add(layers.Dense(80,activation = 'relu',kernel_regularizer = regularizers.l2(0.3))) #adjusted hyperparameter and applied L2 regularization\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))  \n",
    "#experiment with small units\n",
    "\n",
    "\n",
    "model.compile(optimizer = optimizers.Adamax(lr = 0.0007), #adjusted hyperparameters\n",
    "              loss = 'binary_crossentropy', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "history4 = model.fit(trainD, \n",
    "                    trainL,\n",
    "                    validation_split = 0.4,\n",
    "                    epochs = 30, #adjusted hyperparameter\n",
    "                    batch_size =200)  #adjusted hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLoss,testAcc = model.evaluate(testD, testL)    #test set\n",
    "print(\"loss\", testLoss)\n",
    "print(\"Acc\", testAcc)\n",
    "model.predict(testD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set for the large model in this experiment has improved. Like for the small model, The loss has reduced drastically because in experiment 1 the test loss was ≈ 1.26, but in our second try the test loss has now minimized to ≈ 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Experiment 1: results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history1_dict = history1.history\n",
    "loss = history1_dict['loss']\n",
    "val_loss = history1_dict['val_loss']\n",
    "\n",
    "acc = history1_dict['acc']\n",
    "val_acc = history1_dict['val_acc']\n",
    "\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss, 'b', label = 'Training Loss')  \n",
    "plt.plot(epochs, val_loss, 'r', label = 'Validation Loss')  #this subplot should print the training loss and validation loss for \n",
    "plt.title(\"Small Model: Training and validation loss\")      #the small model in experiment 1\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, acc, 'b', label = 'Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label = 'Validation Accuracy')  #this subplot should print the training accuracy and validation \n",
    "plt.title(\"Small Model: Training and validation accuracy\")     #accuracy for the small model in experment 1\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplots_adjust(left=0.1, bottom=1.4, right=2, top=2.5, wspace=None, hspace=None)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "history2_dict = history2.history\n",
    "loss2 = history2_dict['loss']\n",
    "val_loss2 = history2_dict['val_loss']\n",
    "acc2 = history2_dict['acc']\n",
    "val_acc2 = history2_dict['val_acc']\n",
    "\n",
    "epochs2 = range(1, len(loss2) + 1)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs2, loss2, 'b', label = 'Training Loss')\n",
    "plt.plot(epochs2, val_loss2, 'r', label = 'Validation Loss') #this subplot should print the training loss and validation loss for\n",
    "plt.title(\"Large Model: Training and validation loss\")       #the large model in experiment 1\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs2, acc2, 'b', label = 'Training Accuracy')\n",
    "plt.plot(epochs2, val_acc2, 'r', label = 'Validation Accuracy') #this subplot should print the training accuracy and validation\n",
    "plt.title(\"Large Model: Training and validation accuracy\")      #accuracy for the large model in experment 1\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplots_adjust(left=0.1, bottom=1.4, right=2, top=2.5, wspace=None, hspace=None)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, val_loss, 'b', label = 'Small Model Validation Loss')\n",
    "plt.plot(epochs2, val_loss2, 'r', label = 'Large Model Validation Loss') \n",
    "plt.title(\"Comparing validation loss\")    #this subplot should print the validation loss for both               \n",
    "plt.xlabel('Epochs')                      #the large model and the small model for comparison in experiment 1\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, val_acc, 'b', label = 'Small Model Validation Accuracy')  \n",
    "plt.plot(epochs2, val_acc2, 'r', label = 'Large Model Validation Accuracy')\n",
    "plt.title(\"Comparing validation accuracy\")  #this subplot should print the validation accuracy for both\n",
    "plt.xlabel('Epochs')                        #the large model and the small model for comparison in experiment 1      \n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplots_adjust(left=0.1, bottom=1.4, right=2, top=2.5, wspace=None, hspace=None)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy2 = np.empty((len(epochs),2))\n",
    "error2 = np.empty((len(epochs),2))\n",
    "\n",
    "\n",
    "for i in range(len(error2)):\n",
    "    error2[i][0] = loss[i]\n",
    "    error2[i][1] = val_loss[i]\n",
    "\n",
    "\n",
    "for i in range(len(accuracy2)):\n",
    "    accuracy2[i][0] = acc[i]\n",
    "    accuracy2[i][1] = val_acc[i]\n",
    "\n",
    "ax = plt.subplot(111, frame_on=False) \n",
    "ax.xaxis.set_visible(False)    \n",
    "ax.yaxis.set_visible(False) \n",
    "\n",
    "plt.table(cellText=np.around(error2,4), colLabels=[\"Small Model: Training Loss\",\"Small Model: Validation Loss\"],loc = \"left\") \n",
    "plt.table(cellText=np.around(accuracy2,4), colLabels=[\"Small Model: Training Accuracy\",\"Small Model: Validation Accuracy\"],loc = \"right\") \n",
    "\n",
    "plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0.1, hspace=0.1)\n",
    "\n",
    "#this subplot should create two tables\n",
    "#table 1: the training loss and validation loss for the small model in experiment 1\n",
    "#table 2: the training accuracy and validation accuracy for the small model in experiment 1\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "accuracy2 = np.empty((len(epochs2),2))\n",
    "error2 = np.empty((len(epochs2),2))\n",
    "\n",
    "\n",
    "for i in range(len(error2)):\n",
    "    error2[i][0] = loss2[i]\n",
    "    error2[i][1] = val_loss2[i]\n",
    "\n",
    "\n",
    "for i in range(len(accuracy2)):\n",
    "    accuracy2[i][0] = acc2[i]\n",
    "    accuracy2[i][1] = val_acc2[i]\n",
    "\n",
    "ax = plt.subplot(111, frame_on=False) \n",
    "ax.xaxis.set_visible(False) \n",
    "ax.yaxis.set_visible(False) \n",
    "\n",
    "plt.table(cellText=np.around(error2,4), colLabels=[\"Large Model: Training Loss\",\"Large Model: Validation Loss\"],loc = \"left\") \n",
    "\n",
    "plt.table(cellText=np.around(accuracy2,4), colLabels=[\"Large Model: Training Accuracy\",\"Large Model: Validation Accuracy\"],loc = \"right\") \n",
    "\n",
    "plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0.1, hspace=0.1)\n",
    "\n",
    "#this subplot should create two tables\n",
    "#table 1: the training loss and validation loss for the large model in experiment 1\n",
    "#table 2: the training accuracy and validation accuracy for the large model in experiment 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Experiment 1\n",
    "\n",
    "The results shown in my first experiment tell us that both the small model and the large model is overfitting. This is because the validation loss is increasing and the training loss is decreasing at the same time after every epoch. From the results, it seems like the large model overfits much earlier and its validation loss is increasing at a much greater rate than the small model.\n",
    "\n",
    "One way we can prevent overfitting and improving the network structure is by adjusting the hyperparameters. An example of a hyperparameter we can adjust is the learning rate, which manages how much we are changing the weights of the network with regards to the loss gradient. The smaller the value, the more reliable training is, but it will take longer to converge into something useful. Another example of a hyperparameter we can adjust is the number of units within each layer. A small unit would reduce the number of parameters and a large unit would increase the number of parameters. \n",
    "\n",
    "But the main way we can avoid overfitting is by applying a weight regularization technique. Regularization is simply any changes we make to the algorithm that is meant to minimize the generalization error, yet not its training error. The L2 weight regularization is something that can be easily implemented in Keras. It can minimize the weights to a smaller value or can turn the value/s into a zero.\n",
    "\n",
    "In my second experiment, I have adjusted the hyperparameters to what was mentioned in the table specified in the \"Method\" section. I have also applied the L2 weight regularization technique to prevent overfitting. Again, I have tested with one large model and one small model to analyse the differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3_dict = history3.history\n",
    "loss3 = history3_dict['loss']\n",
    "val_loss3 = history3_dict['val_loss']\n",
    "acc3 = history3_dict['acc']\n",
    "val_acc3 = history3_dict['val_acc']\n",
    "\n",
    "epochs3 = range(1, len(loss3) + 1)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs3, loss3, 'b', label = 'Training Loss')\n",
    "plt.plot(epochs3, val_loss3, 'r', label = 'Validation Loss') \n",
    "plt.title(\"Small Model: Training and validation loss\")\n",
    "plt.xlabel('Epochs')  #this subplot should print the training loss and validation loss for\n",
    "plt.ylabel('Loss')    #the small model in experiment 2\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs3, acc3, 'b', label = 'Training Accuracy')\n",
    "plt.plot(epochs3, val_acc3, 'r', label = 'Validation Accuracy')\n",
    "plt.title(\"Small Model: Training and validation accuracy\") \n",
    "plt.xlabel('Epochs')  #this subplot should print the training accuracy and validation accuracy for \n",
    "plt.ylabel('Accuracy') #the small model in experiment 2\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplots_adjust(left=0.1, bottom=1.4, right=2, top=2.5, wspace=None, hspace=None)\n",
    "\n",
    "#this subplot should create two tables\n",
    "#table 1: the training loss and validation loss for the large model in experiment 1\n",
    "#table 2: the training accuracy and validation accuracy for the large model in experiment 1\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "history4_dict = history4.history\n",
    "loss4 = history4_dict['loss']\n",
    "val_loss4 = history4_dict['val_loss']\n",
    "acc4 = history4_dict['acc']\n",
    "val_acc4 = history4_dict['val_acc']\n",
    "\n",
    "epochs4 = range(1, len(loss4) + 1)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs4, loss4, 'b', label = 'Training Loss')\n",
    "plt.plot(epochs4, val_loss4, 'r', label = 'Validation Loss')\n",
    "plt.title(\"Large Model: Training and validation loss\")\n",
    "plt.xlabel('Epochs')  #this subplot should print the training loss and validation loss for\n",
    "plt.ylabel('Loss')    #the large model in experiment 2\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs4, acc4, 'b', label = 'Training Accuracy')\n",
    "plt.plot(epochs4, val_acc4, 'r', label = 'Validation Accuracy')\n",
    "plt.title(\"Large Model: Training and validation accuracy\")\n",
    "plt.xlabel('Epochs') #this subplot should print the training accuracy and validation accuracy for\n",
    "plt.ylabel('Accuracy') #the large model in experiment 2\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplots_adjust(left=0.1, bottom=1.4, right=2, top=2.5, wspace=None, hspace=None)\n",
    "\n",
    "#this subplot should create two tables\n",
    "#table 1: the training loss and validation loss for the large model in experiment 1\n",
    "#table 2: the training accuracy and validation accuracy for the large model in experiment 1\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs3, val_loss3, 'b', label = 'Small Model Validation Loss')\n",
    "plt.plot(epochs4, val_loss4, 'r', label = 'Large Model Validation Loss')\n",
    "plt.title(\"Comparing validation loss\")\n",
    "plt.xlabel('Epochs')  #this subplot should print the validation loss for both\n",
    "plt.ylabel('Loss')    #the large model and the small model for comparison in experiment 2\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs3, val_acc3, 'b', label = 'Small Model Validation Accuracy')\n",
    "plt.plot(epochs4, val_acc4, 'r', label = 'Large Model Validation Accuracy')\n",
    "plt.title(\"Comparing validation accuracy\")\n",
    "plt.xlabel('Epochs')    #this subplot should print the validation accuracy for both\n",
    "plt.ylabel('Accuracy')  #the large model and the small model for comparison in experiment 2\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplots_adjust(left=0.1, bottom=1.4, right=2, top=2.5, wspace=None, hspace=None)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy2 = np.empty((len(epochs3),2))\n",
    "error2 = np.empty((len(epochs3),2))\n",
    "\n",
    "\n",
    "for i in range(len(error2)):\n",
    "    error2[i][0] = loss3[i]\n",
    "    error2[i][1] = val_loss3[i]\n",
    "\n",
    "\n",
    "for i in range(len(accuracy2)):\n",
    "    accuracy2[i][0] = acc3[i]\n",
    "    accuracy2[i][1] = val_acc3[i]\n",
    "\n",
    "ax = plt.subplot(111, frame_on=False) \n",
    "ax.xaxis.set_visible(False) \n",
    "ax.yaxis.set_visible(False) \n",
    "\n",
    "plt.table(cellText=np.around(error2,4), colLabels=[\"Small Model: Training Loss\",\"Small Model: Validation Loss\"],loc = \"left\") \n",
    "plt.table(cellText=np.around(accuracy2,4), colLabels=[\"Small Model: Training Accuracy\",\"Small Model: Validation Accuracy\"],loc = \"right\") \n",
    "\n",
    "plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0.1, hspace=0.1)\n",
    "\n",
    "#this subplot should create two tables\n",
    "#table 1: the training loss and validation loss for the small model in experiment 2\n",
    "#table 2: the training accuracy and validation accuracy for the small model in experiment 2\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "accuracy2 = np.empty((len(epochs4),2))\n",
    "error2 = np.empty((len(epochs4),2))\n",
    "\n",
    "\n",
    "for i in range(len(error2)):\n",
    "    error2[i][0] = loss4[i]\n",
    "    error2[i][1] = val_loss4[i]\n",
    "\n",
    "\n",
    "for i in range(len(accuracy2)):\n",
    "    accuracy2[i][0] = acc4[i]\n",
    "    accuracy2[i][1] = val_acc4[i]\n",
    "\n",
    "ax = plt.subplot(111, frame_on=False) \n",
    "ax.xaxis.set_visible(False) \n",
    "ax.yaxis.set_visible(False) \n",
    "\n",
    "plt.table(cellText=np.around(error2,4), colLabels=[\"Large Model: Training Loss\",\"Large Model: Validation Loss\"],loc = \"left\") \n",
    "\n",
    "plt.table(cellText=np.around(accuracy2,4), colLabels=[\"Large Model: Training Accuracy\",\"Large Model: Validation Accuracy\"],loc = \"right\") \n",
    "\n",
    "plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0.1, hspace=0.1)\n",
    "\n",
    "#this subplot should create two tables\n",
    "#table 1: the training loss and validation loss for the large model in experiment 2\n",
    "#table 2: the training accuracy and validation accuracy for the large model in experiment 2\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of experiment 2\n",
    "\n",
    "From the results in experiment 2, I can a see a massive improvement. The graphs illustrate that I managed to prevent overfitting for the small model because both the validation loss and the training loss are low, but the validation loss is slightly higher. This is a definition of a good fitted model. For the large model, the validation loss starts at a very high number and seems to fall rapidly between epoch 1 - 7 to a point where it is much lower than of the small model. All seems to go well until it gets halfway, where the validation loss starts to slightly increase after each epoch. But nevertheless a much better result in comparison to our first experiment. The L2 weight regularization technique played a massive part in achieving better results. The adjustments of hyperparameters also helped.<br>\n",
    "\n",
    "The validation accuracy for the small model seems to be slightly better than the large model. But, the validation accuracy for both models seems to remain constant throughout the test. Their could be many reasons for this, for example it could be that the model has learnt everything it needs to from the validation set. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Just to summarize, I managed to create a binary classification model in my workflow shown step-by-step as to how it was built. I then carried out two experiments. In my first and second experiment, I tested on a small model and a large model. The results from the two models within my first experiment showed that the validation loss was increasing and the training loss was decreasing after every epoch. From this analysis I realised that the models were overfitting, so for the second experiment I decided to adjust the hyperparameters and apply the L2 weight regularization technique. The results from this experiment showed that I was able to prevent overfitting for the small model as the validation loss decreased and was slightly higher than the training loss. As for the large model, the validation loss was reducing drastically at the beginning, but it started to increase slightly halfway up until the end of the last epoch (30). Nevertheless it was a much better result in comparison to the first experiment. The test set also improved as the loss reduced drastically in experiment 2 for the two models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
